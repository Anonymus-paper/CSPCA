import numpy as onp  
import autograd.numpy as np  
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.cross_decomposition import PLSRegression
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from scipy.spatial.distance import cdist
from scipy import linalg
from pymanopt.manifolds import Grassmann, Euclidean, Product
from pymanopt import Problem
from pymanopt.optimizers import SteepestDescent
import pymanopt.function


def rbf_kernel(Y, sigma):
    dist_matrix = cdist(Y, Y, 'euclidean')
    K = np.exp(-dist_matrix**2 / (2 * sigma**2))
    return K


def SPCA_HSIC(X, Y, K, q):
    C = X.T @ K @ X
    eigenvalues, eigenvectors = linalg.eigh(C)
    sorted_indices = np.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[sorted_indices]
    eigenvectors = eigenvectors[:, sorted_indices]
    W = eigenvectors[:, :q]
    return {'W': W, 'eigenvalues': eigenvalues[:q]}

def CSPCA(X, Y, q, lambda_):
    XtX = X.T @ X
    XtY = X.T @ Y
    YtX = Y.T @ X
    C = XtY @ YtX + lambda_ * XtX
    eigenvalues, eigenvectors = linalg.eigh(C)
    sorted_indices = np.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[sorted_indices]
    eigenvectors = eigenvectors[:, sorted_indices]
    W = eigenvectors[:, :q]
    return {'W': W, 'eigenvalues': eigenvalues[:q]}

def run_all_methods_analysis(data, n_splits=1, n_components_list=[2,4,6,8,10],
                           manifold_lambda=0.03, lambda_=0.1, threshold=0.3, seed=2001):
    
    Y = data[:, 0:1] 
    X = data[:, 1:]
    p = X.shape[1]

    results = pd.DataFrame({
        'n_components': n_components_list,
        'variance_explained_pcr': onp.zeros(len(n_components_list)),
        'mse_PCR': onp.zeros(len(n_components_list)),
        'R2_PCR': onp.zeros(len(n_components_list)),
        'covariance_explained_PCR': onp.zeros(len(n_components_list)),
        'variance_explained_pls': onp.zeros(len(n_components_list)),
        'mse_PLS': onp.zeros(len(n_components_list)),
        'R2_PLS': onp.zeros(len(n_components_list)),
        'variance_explained_hsic': onp.zeros(len(n_components_list)),
        'mse_HSIC': onp.zeros(len(n_components_list)),
        'R2_HSIC': onp.zeros(len(n_components_list)),
        'covariance_explained_HSIC': onp.zeros(len(n_components_list)),
        'variance_explained_bair': onp.zeros(len(n_components_list)),
        'mse_Bair': onp.zeros(len(n_components_list)),
        'R2_Bair': onp.zeros(len(n_components_list)),
        'covariance_explained_Bair': onp.zeros(len(n_components_list)),
        'variance_explained_cspca': onp.zeros(len(n_components_list)),
        'mse_CSPCA': onp.zeros(len(n_components_list)),
        'R2_CSPCA': onp.zeros(len(n_components_list)),
        'covariance_explained_CSPCA': onp.zeros(len(n_components_list)),
        'variance_explained_lspca': onp.zeros(len(n_components_list)),
        'mse_LSPCA': onp.zeros(len(n_components_list)),
        'R2_LSPCA': onp.zeros(len(n_components_list)),
        'covariance_explained_LSPCA': onp.zeros(len(n_components_list))
    })

    for i, n_components in enumerate(n_components_list):
        metrics = {key: [] for key in results.columns if key != 'n_components'}

        for _ in range(n_splits):
            n = X.shape[0]
            train_idx = onp.random.choice(n, size=int(0.8 * n), replace=False)
            test_idx = onp.setdiff1d(onp.arange(n), train_idx)

            X_train, X_test = X[train_idx], X[test_idx]
            Y_train, Y_test = Y[train_idx], Y[test_idx]
            scaler_X = StandardScaler()
            X_train = scaler_X.fit_transform(X_train)
            X_test = scaler_X.transform(X_test)
            scaler_Y = StandardScaler()
            Y_train = scaler_Y.fit_transform(Y_train)
            Y_test = scaler_Y.transform(Y_test)
            X_train_np = np.array(X_train)
            X_test_np = np.array(X_test)
            Y_train_np = np.array(Y_train)
            Y_test_np = np.array(Y_test)

            #PCR
            pca = PCA(n_components=n_components)
            X_train_pca = pca.fit_transform(X_train)
            X_test_pca = pca.transform(X_test)
            var_explained = onp.sum(pca.explained_variance_ratio_)
            metrics['variance_explained_pcr'].append(var_explained)
            lr = LinearRegression()
            lr.fit(X_train_pca, Y_train)
            Y_pred = lr.predict(X_test_pca)
            metrics['mse_PCR'].append(mean_squared_error(Y_test, Y_pred))
            metrics['R2_PCR'].append(r2_score(Y_test, Y_pred))
            cov_xy = onp.cov(X_train.T, Y_train.T)[:-1, -1]
            cov_zy = onp.cov(X_train_pca.T, Y_train.T)[:-1, -1]
            metrics['covariance_explained_PCR'].append(onp.sum(cov_zy**2) / onp.sum(cov_xy**2))

            #PLS
            pls = PLSRegression(n_components=n_components)
            pls.fit(X_train, Y_train)
            Y_pred_pls = pls.predict(X_test)
            var_expl_pls = onp.var(pls.x_scores_, axis=0).sum() / onp.var(X_train, axis=0).sum()
            metrics['variance_explained_pls'].append(var_expl_pls)
            metrics['mse_PLS'].append(mean_squared_error(Y_test, Y_pred_pls))
            metrics['R2_PLS'].append(r2_score(Y_test, Y_pred_pls))

            #SPCA_HSIC
            sigma = 0.1
            K = rbf_kernel(Y_train, sigma)
            spca_result = SPCA_HSIC(X_train, Y_train, K, n_components)
            W_hsic = spca_result['W']
            Z_train_hsic = X_train @ W_hsic
            Z_test_hsic = X_test @ W_hsic
            total_var = onp.trace(X_train.T @ X_train)
            expl_var = onp.trace(W_hsic.T @ X_train.T @ X_train @ W_hsic)
            metrics['variance_explained_hsic'].append(expl_var / total_var)
            lr_hsic = LinearRegression().fit(Z_train_hsic, Y_train)
            Y_pred_hsic = lr_hsic.predict(Z_test_hsic)
            metrics['mse_HSIC'].append(mean_squared_error(Y_test, Y_pred_hsic))
            metrics['R2_HSIC'].append(r2_score(Y_test, Y_pred_hsic))
            cov_zy = onp.cov(Z_train_hsic.T, Y_train.T)[:-1, -1]
            metrics['covariance_explained_HSIC'].append(onp.sum(cov_zy**2) / onp.sum(cov_xy**2))

            #Bair's Method
            feature_scores = onp.abs(onp.corrcoef(X_train.T, Y_train.T)[:-1, -1])
            selected_features = feature_scores >= threshold
            X_train_selected = X_train[:, selected_features]
            X_test_selected = X_test[:, selected_features]
            pca_bair = PCA(n_components=min(n_components, X_train_selected.shape[1]))
            X_train_bair = pca_bair.fit_transform(X_train_selected)
            X_test_bair = pca_bair.transform(X_test_selected)
            metrics['variance_explained_bair'].append(onp.sum(pca_bair.explained_variance_ratio_))
            lr_bair = LinearRegression().fit(X_train_bair, Y_train)
            Y_pred_bair = lr_bair.predict(X_test_bair)
            metrics['mse_Bair'].append(mean_squared_error(Y_test, Y_pred_bair))
            metrics['R2_Bair'].append(r2_score(Y_test, Y_pred_bair))
            cov_zy = onp.cov(X_train_bair.T, Y_train.T)[:-1, -1]
            metrics['covariance_explained_Bair'].append(onp.sum(cov_zy**2) / onp.sum(cov_xy**2))

            #CSPCA
            spca_sup = CSPCA(X_train, Y_train, n_components, lambda_)
            W_CSPCA = spca_sup['W']
            Z_train_CSPCA = X_train @ W_CSPCA
            Z_test_CSPCA = X_test @ W_CSPCA
            expl_var_sup = onp.trace(W_CSPCA.T @ X_train.T @ X_train @ W_CSPCA)
            metrics['variance_explained_cspca'].append(expl_var_sup / total_var)
            lr_CSPCA = LinearRegression().fit(Z_train_CSPCA, Y_train)
            Y_pred_CSPCA = lr_CSPCA.predict(Z_test_CSPCA)
            metrics['mse_CSPCA'].append(mean_squared_error(Y_test, Y_pred_CSPCA))
            metrics['R2_CSPCA'].append(r2_score(Y_test, Y_pred_CSPCA))
            cov_zy = onp.cov(Z_train_CSPCA.T, Y_train.T)[:-1, -1]
            metrics['covariance_explained_CSPCA'].append(onp.sum(cov_zy**2) / onp.sum(cov_xy**2))

            #LSPCA
            lspca = Product([Grassmann(p, n_components), Euclidean(n_components, 1)])  
            @pymanopt.function.autograd(manifold)
            def cost(L, w):
                XL = X_train_np @ L
                lr_resid = Y_train_np - (XL @ w)
                pca_resid = X_train_np - (XL @ L.T)
                lr_obj = np.mean(lr_resid ** 2)
                pca_obj = np.mean(pca_resid ** 2)
                return (manifold_lambda * lr_obj) + ((1 - manifold_lambda) * pca_obj)

            problem = Problem(manifold=lspca, cost=cost)
            solver = SteepestDescent(verbosity=0)
            L_init = onp.random.normal(size=(p, n_components))
            w_init = onp.random.normal(size=(n_components, 1))
            result = solver.run(problem, initial_point=[L_init, w_init])
            L_optimized, w_optimized = result.point
            Z_train_lspca = X_train @ L_optimized  
            Z_test_lspca = X_test @ L_optimized
            Y_pred_lspca = Z_test_lspca @ w_optimized
            expl_var_man = onp.trace(L_optimized.T @ X_train.T @ X_train @ L_optimized)
            metrics['variance_explained_lspca'].append(expl_var_man / total_var)
            metrics['mse_LSPCA'].append(mean_squared_error(Y_test, Y_pred_lspca))
            metrics['R2_LSPCA'].append(r2_score(Y_test, Y_pred_lspca))
            cov_zy = onp.cov(Z_train_lspca.T, Y_train.T)[:-1, -1]
            metrics['covariance_explained_LSPCA'].append(onp.sum(cov_zy**2) / onp.sum(cov_xy**2))

        
        for key in metrics:
            results.loc[i, key] = onp.mean(metrics[key])

    return results

#Simulation A: Y = 3X_1 - 5X_2 + 4X_3 + \epsilon, \epsilon\sim N(0,0.1^2)
def run_simulation_A(n_datasets=50, n=100, p=50, n_splits=1, n_components_list=[2,4,6,8,10]):
    methods = ['PCR', 'PLS', 'HSIC', 'Bair', 'CSPCA', 'LSPCA']
    results = {method: {
        'VarExpl': onp.zeros((n_datasets, len(n_components_list))),
        'MSE': onp.zeros((n_datasets, len(n_components_list))),
        'CovExpl': onp.zeros((n_datasets, len(n_components_list))),
        'R2': onp.zeros((n_datasets, len(n_components_list)))
    } for method in methods}

    for i in range(n_datasets):
        X = onp.random.normal(size=(n, p))
        X_informative = X[:, :3]
        true_beta = onp.array([3, -5, 4])
        error = onp.random.normal(0, 0.1, n)
        Y = X_informative @ true_beta + error
        data_A = onp.column_stack((Y[:, onp.newaxis], X))  

        res = run_all_methods_analysis(data_A, n_splits=n_splits, n_components_list=n_components_list)

        for method in methods:
            var_key = 'f'variance_explained_{method.lower()}'
            results[method]['VarExpl'][i] = res[var_key]
            results[method]['MSE'][i] = res[f'mse_{method}']
            results[method]['R2'][i] = res[f'R2_{method}']
            if method != 'PLS':  
                results[method]['CovExpl'][i] = res[f'covariance_explained_{method}']

   
    def compute_stats(metric_matrix):
        avg = onp.nanmean(metric_matrix, axis=0)
        std_err = onp.nanstd(metric_matrix, axis=0) / onp.sqrt(n_datasets)
        return {'avg': avg, 'std_err': std_err}

    stats = {method: {
        'VarExpl': compute_stats(results[method]['VarExpl']),
        'MSE': compute_stats(results[method]['MSE']),
        'CovExpl': compute_stats(results[method]['CovExpl']),
        'R2': compute_stats(results[method]['R2'])
    } for method in methods}

    
    print("\nSimulation A (Linear: Y = 3X1 - 5X2 + 4X3 + error):")
    for method in methods:
        print(f"  {method}:")
        for metric in ['Variance Explained', 'MSE', 'Covariance Explained', 'R2']:
            print(f"    {metric}:")
            for j, n_comp in enumerate(n_components_list):
                key = metric.split()[0][:3] + ('Expl' if 'Explained' in metric else '')
                avg = stats[method][key]['avg'][j]
                se = stats[method][key]['std_err'][j]
                print(f"      n_components = {n_comp}: Avg = {avg:.4f} ± {se:.4f}")

    return {'stats': stats, 'raw_results': results}


onp.random.seed(1924)
results_A = run_simulation_A(n_datasets=50, n=100, p=50, n_splits=1, n_components_list=[2,4,6,8,10])
