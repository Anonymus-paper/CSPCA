import numpy as onp  
import autograd.numpy as np 
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.cross_decomposition import PLSRegression
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from scipy.spatial.distance import cdist
from scipy import linalg
!pip install pymanopt
from pymanopt.manifolds import Grassmann, Euclidean, Product
from pymanopt import Problem
from pymanopt.optimizers import SteepestDescent
import pymanopt.function
import numpy as onp
import autograd.numpy as np
from scipy import linalg
from scipy.linalg import svd, qr, pinv
from sklearn.model_selection import train_test_split


Y_df = pd.read_csv('liver_toxicity_Y.csv')
X_df = pd.read_csv('liver_toxicity_X.csv')
Y = Y_df.values
X = X_df.values
if len(Y.shape) == 1 or Y.shape[1] == 1:
    Y = Y.reshape(-1, 1)
print("Y shape:", Y.shape)
print("X shape:", X.shape)



def rbf_kernel(Y, sigma):
    dist_matrix = cdist(Y, Y, 'euclidean')
    K = np.exp(-dist_matrix**2 / (2 * sigma**2))
    return K


def DUAL_HSIC(X, Y, K, q):
    Delta = linalg.cholesky(K, lower=True)  
    psi = X.T @ Delta  # Shape: (p x n)
    U, s, Vt = svd(psi, full_matrices=False)
    Sigma = onp.diag(s)
    U_d = U[:, :q]
    Sigma_d = Sigma[:q, :q]
    V_d = Vt.T[:, :q]  # Vt is (n x p), so V_d is (p x q)
    U_hat = psi @ V_d @ linalg.inv(Sigma_d)  # Shape: (p x q)
    eigenvalues = s[:q]**2  # Singular values squared

    return {'W': U_hat, 'singular_values': eigenvalues}

def CSPCA_Nys(X, Y, q, p, m, lambda_):
    XtY = X.T @ Y
    YtX = Y.T @ X
    XtX = X.T @ X
    C = XtY @ YtX + lambda_ * XtX  
    indices = onp.random.choice(p, size=m, replace=False)
    C_nm = C[:, indices]  # Shape: (p x m)
    C_mm = C[indices, :][:, indices]  # Shape: (m x m)
    eigenvalues, U_m = linalg.eigh(C_mm)  
    sorted_indices = onp.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[sorted_indices]
    U_m = U_m[:, sorted_indices]
    Lambda_m = onp.diag(eigenvalues)
    Lambda_m_inv_sqrt = onp.diag(1.0 / onp.sqrt(onp.maximum(eigenvalues, 1e-6)))
    U_n = C_nm @ U_m @ Lambda_m_inv_sqrt  # Shape: (p x m)
    C_mm_inv = pinv(C_mm)  
    C_approx = C_nm @ C_mm_inv @ C_nm.T
    W = U_n[:, :q]
    W, _ = qr(W, mode='economic')  

    return {'W': W, 'eigenvalues': eigenvalues[:q]}



def run_all_methods_analysis(X, Y, n_splits=10, n_components_list=[2,4,6,8,10],
                           manifold_lambda=0.05, lambda_=0.01, threshold=0.3, seed=1884):
    
    
    if len(Y.shape) == 1:
        Y = Y[:, onp.newaxis]
    data = onp.column_stack((Y, X))
    Y = data[:, 0:1]  
    X = data[:, 1:]
    p = X.shape[1]

    results = pd.DataFrame({
        'n_components': n_components_list,
        'variance_explained_pcr': onp.zeros(len(n_components_list)),
        'mse_PCR': onp.zeros(len(n_components_list)),
        'R2_PCR': onp.zeros(len(n_components_list)),
        'covariance_explained_PCR': onp.zeros(len(n_components_list)),
        'variance_explained_pls': onp.zeros(len(n_components_list)),
        'mse_PLS': onp.zeros(len(n_components_list)),
        'R2_PLS': onp.zeros(len(n_components_list)),
        'variance_explained_hsic': onp.zeros(len(n_components_list)),
        'mse_HSIC': onp.zeros(len(n_components_list)),
        'R2_HSIC': onp.zeros(len(n_components_list)),
        'covariance_explained_HSIC': onp.zeros(len(n_components_list)),
        'variance_explained_bair': onp.zeros(len(n_components_list)),
        'mse_Bair': onp.zeros(len(n_components_list)),
        'R2_Bair': onp.zeros(len(n_components_list)),
        'covariance_explained_Bair': onp.zeros(len(n_components_list)),
        'variance_explained_cspca': onp.zeros(len(n_components_list)),
        'mse_CSPCA': onp.zeros(len(n_components_list)),
        'R2_CSPCA': onp.zeros(len(n_components_list)),
        'covariance_explained_CSPCA': onp.zeros(len(n_components_list)),
        'variance_explained_lspca': onp.zeros(len(n_components_list)),
        'mse_LSPCA': onp.zeros(len(n_components_list)),
        'R2_LSPCA': onp.zeros(len(n_components_list)),
        'covariance_explained_LSPCA': onp.zeros(len(n_components_list))
    })


    for i, n_components in enumerate(n_components_list):
        metrics = {key: [] for key in results.columns if key != 'n_components'}

        for _ in range(n_splits):
            n = X.shape[0]
            p = X.shape[1]
            X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1924)
            scaler_X = StandardScaler()
            X_train = scaler_X.fit_transform(X_train)
            X_test = scaler_X.transform(X_test)
            mean_Y_train = np.mean(Y_train)
            std_Y_train = np.std(Y_train)
            Y_train = (Y_train - mean_Y_train) / std_Y_train
            Y_test = (Y_test - mean_Y_train) / std_Y_train
            X_train_np = np.array(X_train)
            X_test_np = np.array(X_test)
            Y_train_np = np.array(Y_train)
            Y_test_np = np.array(Y_test)

            #PCR
            pca = PCA(n_components=n_components)
            X_train_pca = pca.fit_transform(X_train)
            X_test_pca = pca.transform(X_test)
            var_explained = onp.sum(pca.explained_variance_ratio_)
            metrics['variance_explained_pcr'].append(var_explained)
            lr = LinearRegression()
            lr.fit(X_train_pca, Y_train)
            Y_pred = lr.predict(X_test_pca)
            metrics['mse_PCR'].append(mean_squared_error(Y_test, Y_pred))
            metrics['R2_PCR'].append(r2_score(Y_test, Y_pred))
            cov_xy = onp.cov(X_train.T, Y_train.T)[:-1, -1]
            cov_zy = onp.cov(X_train_pca.T, Y_train.T)[:-1, -1]
            metrics['covariance_explained_PCR'].append(onp.sum(cov_zy**2) / onp.sum(cov_xy**2))

            #PLS
            pls = PLSRegression(n_components=n_components)
            pls.fit(X_train, Y_train)
            Y_pred_pls = pls.predict(X_test)
            var_expl_pls = onp.var(pls.x_scores_, axis=0).sum() / onp.var(X_train, axis=0).sum()
            metrics['variance_explained_pls'].append(var_expl_pls)
            metrics['mse_PLS'].append(mean_squared_error(Y_test, Y_pred_pls))
            metrics['R2_PLS'].append(r2_score(Y_test, Y_pred_pls))

            #DUAL HSIC
            sigma = 0.1
            K = rbf_kernel(Y_train, sigma)
            epsilon = 1e-15 
            K += epsilon * np.eye(K.shape[0])
            result = DUAL_HSIC(X_train, Y_train, K, n_components)
            W_hsic = result['W']
            Z_train_hsic = X_train @ W_hsic
            Z_test_hsic = X_test @ W_hsic
            total_var = onp.trace(X_train.T @ X_train)
            expl_var = onp.trace(W_hsic.T @ X_train.T @ X_train @ W_hsic)
            metrics['variance_explained_hsic'].append(expl_var / total_var)
            lr_hsic = LinearRegression().fit(Z_train_hsic, Y_train)
            Y_pred_hsic = lr_hsic.predict(Z_test_hsic)
            metrics['mse_HSIC'].append(mean_squared_error(Y_test, Y_pred_hsic))
            metrics['R2_HSIC'].append(r2_score(Y_test, Y_pred_hsic))
            cov_zy = onp.cov(Z_train_hsic.T, Y_train.T)[:-1, -1]
            metrics['covariance_explained_HSIC'].append(onp.sum(cov_zy**2) / onp.sum(cov_xy**2))



            #Bair's Method
            feature_scores = onp.abs(onp.corrcoef(X_train.T, Y_train.T)[:-1, -1])
            selected_features = feature_scores >= threshold
            X_train_selected = X_train[:, selected_features]
            X_test_selected = X_test[:, selected_features]
            pca_bair = PCA(n_components=min(n_components, X_train_selected.shape[1]))
            X_train_bair = pca_bair.fit_transform(X_train_selected)
            X_test_bair = pca_bair.transform(X_test_selected)
            metrics['variance_explained_bair'].append(onp.sum(pca_bair.explained_variance_ratio_))
            lr_bair = LinearRegression().fit(X_train_bair, Y_train)
            Y_pred_bair = lr_bair.predict(X_test_bair)
            metrics['mse_Bair'].append(mean_squared_error(Y_test, Y_pred_bair))
            metrics['R2_Bair'].append(r2_score(Y_test, Y_pred_bair))
            cov_zy = onp.cov(X_train_bair.T, Y_train.T)[:-1, -1]
            metrics['covariance_explained_Bair'].append(onp.sum(cov_zy**2) / onp.sum(cov_xy**2))

            #CSPCA using Nystrom
            cspca_res = CSPCA_Nym(X_train, Y_train, n_components, p=p, m=200, lambda_=lambda_)
            W = cspca_res['W']
            Z_train_cspca = X_train @ W
            Z_test_cspca = X_test @ W
            expl_var_sup = onp.trace(W.T @ X_train.T @ X_train @ W)
            metrics['variance_explained_cspca'].append(expl_var_sup / total_var)
            lr_cspca = LinearRegression().fit(Z_train_cspca, Y_train)
            Y_pred_cspca = lr_cspca.predict(Z_test_cspca)
            metrics['mse_CSPCA'].append(mean_squared_error(Y_test, Y_pred_cspca))
            metrics['R2_CSPCA'].append(r2_score(Y_test, Y_pred_cspca))
            cov_zy = onp.cov(Z_train_cspca.T, Y_train.T)[:-1, -1]
            metrics['covariance_explained_CSPCA'].append(onp.sum(cov_zy**2) / onp.sum(cov_xy**2))

            # Manifold Optimization (LSPCA)
            lspca = Product([Grassmann(p, n_components), Euclidean(n_components, 1)])  
            @pymanopt.function.autograd(manifold)
            def cost(L, w):
                XL = X_train_np @ L
                lr_resid = Y_train_np - (XL @ w)
                pca_resid = X_train_np - (XL @ L.T)
                lr_obj = np.mean(lr_resid ** 2)
                pca_obj = np.mean(pca_resid ** 2)
                return (manifold_lambda * lr_obj) + ((1 - manifold_lambda) * pca_obj)

            problem = Problem(manifold=lspca, cost=cost)
            solver = SteepestDescent(verbosity=0)
            L_init = onp.random.normal(size=(p, n_components))
            w_init = onp.random.normal(size=(n_components, 1))
            result = solver.run(problem, initial_point=[L_init, w_init])
            L_optimized, w_optimized = result.point
            Z_train_man = X_train @ L_optimized  # Use regular NumPy for subsequent operations
            Z_test_man = X_test @ L_optimized
            Y_pred_man = Z_test_man @ w_optimized
            expl_var_man = onp.trace(L_optimized.T @ X_train.T @ X_train @ L_optimized)
            metrics['variance_explained_lspca'].append(expl_var_man / total_var)
            metrics['mse_LSPCA'].append(mean_squared_error(Y_test, Y_pred_man))
            metrics['R2_LSPCA'].append(r2_score(Y_test, Y_pred_man))
            cov_zy = onp.cov(Z_train_man.T, Y_train.T)[:-1, -1]
            metrics['covariance_explained_LSPCA'].append(onp.sum(cov_zy**2) / onp.sum(cov_xy**2))

       
        for key in metrics:
            results.loc[i, key] = onp.mean(metrics[key])

    return results

def analyze_real_data(X, Y, n_splits=10, n_components_list=[2,4,6,8,10]):
    methods = ['PCR', 'PLS', 'HSIC', 'Bair', 'CSPCA', 'LSPCA']

    
    results = run_all_methods_analysis(X, Y, n_splits=n_splits, n_components_list=n_components_list)

    
    stats = {}
    for method in methods:
        stats[method] = {
            'VarExpl': {
                'avg': results[f'variance_explained_{method.lower()}'].values,
                'std_err': onp.zeros(len(n_components_list)) 
            },
            'MSE': {
                'avg': results[f'mse_{method}'].values,
                'std_err': onp.zeros(len(n_components_list))
            },
            'R2': {
                'avg': results[f'R2_{method}'].values,
                'std_err': onp.zeros(len(n_components_list))
            },
            'CovExpl': {
                'avg': results[f'covariance_explained_{method}'].values,
                'std_err': onp.zeros(len(n_components_list))
            }
        }

    
    print(f"\nReal Data Analysis (n_splits={n_splits}):")
    for method in methods:
        print(f"  {method}:")
        for metric in ['Variance Explained', 'MSE', 'Covariance Explained', 'R2']:
            print(f"    {metric}:")
            for j, n_comp in enumerate(n_components_list):
                key = metric.split()[0][:3] + ('Expl' if 'Explained' in metric else '')
                avg = stats[method][key]['avg'][j]
                print(f"      n_components = {n_comp}: Avg = {avg:.4f}")

    return {'stats': stats, 'raw_results': results}

